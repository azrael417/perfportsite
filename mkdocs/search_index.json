{
    "docs": [
        {
            "location": "/", 
            "text": "some information\n\n\nJack / Bronson / Tim / Scott to write\n\n\nWebsite for Office of Science Performance Portability Across ALCF, NERSC, OLCF Facilities\n\n\nThe need for applications to run effectively on multiple vendor advanced architecture solutions (as well as on standard \"cluster\" technology) is pervasive across application teams within DOE and is a specified goal of the DOE's exascale plans for risk mitigation.\n\n\nRecognizing the immense challenges of porting and optimizing large applications to the advanced architecture systems planned for deployment within the Nationa Nuclear Security Administration (NNSA) and Office of Science (SC) labs between 2016 and 2019, the DOE has established a COE at each laboratory siting one of these systems. These COEs provide direct vendor expertise to the application teams and in turn, give the vendors deeper insight into how applications are run on those systems. Each of the five current COEs has a mission to optimize a set of applications for their specific platform\u2014however the application teams are motivated to maintain a code base that will run effectively across diverse vendor offerings. Making use of open standards, libraries, and software abstractions that allow for minimal code disruption without negatively impacting performance potential is the preferred path to programming, but it constitutes a large, as-yet-unsolved challenge.\n(Text shamelessly copied from \nhere\n)", 
            "title": "Introduction"
        }, 
        {
            "location": "/#some-information", 
            "text": "Jack / Bronson / Tim / Scott to write  Website for Office of Science Performance Portability Across ALCF, NERSC, OLCF Facilities  The need for applications to run effectively on multiple vendor advanced architecture solutions (as well as on standard \"cluster\" technology) is pervasive across application teams within DOE and is a specified goal of the DOE's exascale plans for risk mitigation.  Recognizing the immense challenges of porting and optimizing large applications to the advanced architecture systems planned for deployment within the Nationa Nuclear Security Administration (NNSA) and Office of Science (SC) labs between 2016 and 2019, the DOE has established a COE at each laboratory siting one of these systems. These COEs provide direct vendor expertise to the application teams and in turn, give the vendors deeper insight into how applications are run on those systems. Each of the five current COEs has a mission to optimize a set of applications for their specific platform\u2014however the application teams are motivated to maintain a code base that will run effectively across diverse vendor offerings. Making use of open standards, libraries, and software abstractions that allow for minimal code disruption without negatively impacting performance potential is the preferred path to programming, but it constitutes a large, as-yet-unsolved challenge.\n(Text shamelessly copied from  here )", 
            "title": "some information"
        }, 
        {
            "location": "/facilities/overview/", 
            "text": "Jack / Bronson / Tim to Write", 
            "title": "Overview"
        }, 
        {
            "location": "/facilities/tools/", 
            "text": "Brian / ? / ? to Write", 
            "title": "Tools"
        }, 
        {
            "location": "/facilities/comparison/", 
            "text": "", 
            "title": "Comparison"
        }, 
        {
            "location": "/facilities/resources/", 
            "text": "Jack / Bronson / Tim to Write", 
            "title": "Resources"
        }, 
        {
            "location": "/perfport/definition/", 
            "text": "Jack To Write", 
            "title": "Definition"
        }, 
        {
            "location": "/perfport/measurements/", 
            "text": "Jack To Write", 
            "title": "Measurements"
        }, 
        {
            "location": "/perfport/libraries/", 
            "text": "Bronson / ORNL to Write", 
            "title": "Libraries"
        }, 
        {
            "location": "/perfport/directives/", 
            "text": "", 
            "title": "Directives"
        }, 
        {
            "location": "/perfport/frameworks/", 
            "text": "", 
            "title": "Frameworks"
        }, 
        {
            "location": "/perfport/dsl/", 
            "text": "", 
            "title": "DSL"
        }, 
        {
            "location": "/perfport/models/", 
            "text": "", 
            "title": "Models"
        }, 
        {
            "location": "/case_studies/amr/overview/", 
            "text": "Thorsten / Brian to Write\n\n\nOverview of BoxLib/AMReX\n\n\nBoxLib\n is a framework for developing\nparallel, block-structured, adaptive mesh refinement (AMR) applications. It is\nwritten primarily in C++, and enables scientific application development\nthrough compute kernels written primarily in Fortran. Through the \nExascale\nComputing Project\n's\n\nBlock Structured Adaptive Mesh Refinement Co-Design\nCenter\n,\nBoxLib has since been superseded by\n\nAMReX\n. Both frameworks are publicly\navailable. The DOE COE for Performance Portability began prior to the formation\nof the Co-Design Center; consequently,the efforts described here focus on\nBoxLib, although the functionality described here is largely the same between\nthe two frameworks.\n\n\nBoxLib contains a wide variety of functionality:\n\n\n\n\nboundary condition exchange among boxes\n\n\nload balancing through regridding boxes among MPI processes\n\n\nmetadata operations such as computing volume intersections among boxes\n\n\nmemory management through pool allocators\n\n\n\n\nIn addition these, BoxLib also provides linear solvers which use geometric\nmultigrid methods to solve problems on both cell-centered and nodal data. Our\nperformance portability efforts described here focus on the cell-centered\nsolver, which is algorithmically simpler than the nodal solver.", 
            "title": "Overview"
        }, 
        {
            "location": "/case_studies/amr/overview/#overview-of-boxlibamrex", 
            "text": "BoxLib  is a framework for developing\nparallel, block-structured, adaptive mesh refinement (AMR) applications. It is\nwritten primarily in C++, and enables scientific application development\nthrough compute kernels written primarily in Fortran. Through the  Exascale\nComputing Project 's Block Structured Adaptive Mesh Refinement Co-Design\nCenter ,\nBoxLib has since been superseded by AMReX . Both frameworks are publicly\navailable. The DOE COE for Performance Portability began prior to the formation\nof the Co-Design Center; consequently,the efforts described here focus on\nBoxLib, although the functionality described here is largely the same between\nthe two frameworks.  BoxLib contains a wide variety of functionality:   boundary condition exchange among boxes  load balancing through regridding boxes among MPI processes  metadata operations such as computing volume intersections among boxes  memory management through pool allocators   In addition these, BoxLib also provides linear solvers which use geometric\nmultigrid methods to solve problems on both cell-centered and nodal data. Our\nperformance portability efforts described here focus on the cell-centered\nsolver, which is algorithmically simpler than the nodal solver.", 
            "title": "Overview of BoxLib/AMReX"
        }, 
        {
            "location": "/case_studies/amr/parallelism/", 
            "text": "Parallelization\n\n\nBoxLib implements parallelization through a hybrid MPI+OpenMP approach.\n\n\nMPI\n\n\nAt the coarsest level, BoxLib decomposes the problem domain into rectangular\nboxes, and distributes these among MPI processes. Each process follows an\n\"owner computes\" model, wherein it loops over its own boxes, executing Fortran\nkernels on each box in series.\n\n\nOpenMP\n\n\nBoxLib adds an additional layer of parallelism within each MPI process through\nOpenMP threading, specifically by decomposing its set of boxes into a set of\nsmaller \"tiles\", which are then distributed among OpenMP threads. Although\nthese tiles can be arbitrarily shaped, by default they are pencil-shaped, being\nlong in the stride-1 memory access dimension (the x-dimension in Fortran\nkernels), and short in the other two dimensions, in order to attain high cache\nreuse and optimal hardware memory prefetching. As with the MPI parallelism, the\nOpenMP tile box parallelism also follows an \"owner computes\" model, but at the\nfiner-grained thread level, rather than at the process level.", 
            "title": "Parallelism"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#parallelization", 
            "text": "BoxLib implements parallelization through a hybrid MPI+OpenMP approach.", 
            "title": "Parallelization"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#mpi", 
            "text": "At the coarsest level, BoxLib decomposes the problem domain into rectangular\nboxes, and distributes these among MPI processes. Each process follows an\n\"owner computes\" model, wherein it loops over its own boxes, executing Fortran\nkernels on each box in series.", 
            "title": "MPI"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#openmp", 
            "text": "BoxLib adds an additional layer of parallelism within each MPI process through\nOpenMP threading, specifically by decomposing its set of boxes into a set of\nsmaller \"tiles\", which are then distributed among OpenMP threads. Although\nthese tiles can be arbitrarily shaped, by default they are pencil-shaped, being\nlong in the stride-1 memory access dimension (the x-dimension in Fortran\nkernels), and short in the other two dimensions, in order to attain high cache\nreuse and optimal hardware memory prefetching. As with the MPI parallelism, the\nOpenMP tile box parallelism also follows an \"owner computes\" model, but at the\nfiner-grained thread level, rather than at the process level.", 
            "title": "OpenMP"
        }, 
        {
            "location": "/case_studies/amr/code_layout/", 
            "text": "Code Layout\n\n\nThe typical form of a BoxLib application is a C++ \"driver\" code which manages\nthe boxes on the domain, and calls Fortran kernels in a loop over the boxes\nowned by each MPI process. An example of this layout is shown below:\n\n\n// Advance the solution one grid at a time\n\n\nfor\n \n(\n \nMFIter\n \nmfi\n(\nold_phi\n);\n \nmfi\n.\nisValid\n();\n \n++\nmfi\n \n)\n\n\n{\n\n  \nconst\n \nBox\n \nbx\n \n=\n \nmfi\n.\nvalidbox\n();\n\n\n  \nupdate_phi\n(\nold_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nnew_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nng_p\n,\n\n             \nflux\n[\n0\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n1\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n2\n][\nmfi\n].\ndataPtr\n(),\n\n             \nng_f\n,\n \nbx\n.\nloVect\n(),\n \nbx\n.\nhiVect\n(),\n \ndx\n[\n0\n],\n \ndt\n);\n\n  \n}\n\n\n}\n\n\n\n\n\nHere the \nMFIter\n object is an iterator over boxes owned by an MPI process. The\n\nBox\n object contains the geometric metadata describing a particular box, e.g.,\nthe indices of the lower and upper corners. The variables \nold_phi\n, \nnew_phi\n,\nand \nflux\n contain pointers to the arrays which contain the floating point data\non the grid. The \nupdate_phi\n function is a Fortran function which uses the\ndata from the \nBox\n object to construct 3-D loops over the appropriate section\nof the three floating-point arrays. The function may look like the following:\n\n\nsubroutine \nupdate_phi\n(\nphiold\n,\n \nphinew\n,\n \nng_p\n,\n \nfluxx\n,\n \nfluxy\n,\n \nfluxz\n,\n \nng_f\n,\n \nlo\n,\n \nhi\n,\n \ndx\n,\n \ndt\n)\n \nbind\n(\nC\n,\n \nname\n=\nupdate_phi\n)\n\n\n  \ninteger\n          \n::\n \nlo\n(\n3\n),\n \nhi\n(\n3\n),\n \nng_p\n,\n \nng_f\n\n  \ndouble precision\n \n::\n \nphiold\n(\nlo\n(\n1\n)\n-\nng_p\n:\nhi\n(\n1\n)\n+\nng_p\n,\nlo\n(\n2\n)\n-\nng_p\n:\nhi\n(\n2\n)\n+\nng_p\n,\nlo\n(\n3\n)\n-\nng_p\n:\nhi\n(\n3\n)\n+\nng_p\n)\n\n  \ndouble precision\n \n::\n \nphinew\n(\nlo\n(\n1\n)\n-\nng_p\n:\nhi\n(\n1\n)\n+\nng_p\n,\nlo\n(\n2\n)\n-\nng_p\n:\nhi\n(\n2\n)\n+\nng_p\n,\nlo\n(\n3\n)\n-\nng_p\n:\nhi\n(\n3\n)\n+\nng_p\n)\n\n  \ndouble precision\n \n::\n  \nfluxx\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n+\n1\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n)\n\n  \ndouble precision\n \n::\n  \nfluxy\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n+\n1\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n)\n\n  \ndouble precision\n \n::\n  \nfluxz\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n+\n1\n)\n\n  \ndouble precision\n \n::\n \ndx\n,\n \ndt\n\n\n  \ninteger \ni\n,\nj\n,\nk\n\n\n  \ndo \nk\n=\nlo\n(\n3\n),\nhi\n(\n3\n)\n\n     \ndo \nj\n=\nlo\n(\n2\n),\nhi\n(\n2\n)\n\n        \ndo \ni\n=\nlo\n(\n1\n),\nhi\n(\n1\n)\n\n\n           \nphinew\n(\ni\n,\nj\n,\nk\n)\n \n=\n \nphiold\n(\ni\n,\nj\n,\nk\n)\n \n+\n \ndt\n \n*\n \n\n                \n(\n \nfluxx\n(\ni\n+\n1\n,\nj\n,\nk\n)\n-\nfluxx\n(\ni\n,\nj\n,\nk\n)\n \n\n                \n+\nfluxy\n(\ni\n,\nj\n+\n1\n,\nk\n)\n-\nfluxy\n(\ni\n,\nj\n,\nk\n)\n \n\n                \n+\nfluxz\n(\ni\n,\nj\n,\nk\n+\n1\n)\n-\nfluxz\n(\ni\n,\nj\n,\nk\n)\n \n)\n \n/\n \ndx\n\n\n        \nend do\n\n\n     end do\n\n\n  end do\n\n\n\n\n\nThe Fortran function constructs the appropriate \"view\" into each box using the\ndata from the \nBox\n object from the C++ function, as well as from the number of\nghost zones (\nng_p\n for \nold_phi\n and \nnew_phi\n, and \nng_f\n for \nflux\n).\n\n\nThe above example demonstrates pure MPI parallelism; the analogous C++ code\nwhich uses OpenMP tiling as described above would look like the following:\n\n\n// Advance the solution one grid at a time\n\n\n#ifdef _OPENMP\n\n\n#pragma omp parallel\n\n\n#endif\n\n\nfor\n \n(\n \nMFIter\n \nmfi\n(\nold_phi\n,\ntrue\n);\n \nmfi\n.\nisValid\n();\n \n++\nmfi\n \n)\n\n\n{\n\n  \nconst\n \nBox\n \ntbx\n \n=\n \nmfi\n.\ntilebox\n();\n\n\n  \nupdate_phi\n(\nold_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nnew_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nng_p\n,\n\n             \nflux\n[\n0\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n1\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n2\n][\nmfi\n].\ndataPtr\n(),\n\n             \nng_f\n,\n \ntbx\n.\nloVect\n(),\n \ntbx\n.\nhiVect\n(),\n \ndx\n[\n0\n],\n \ndt\n);\n\n  \n}\n\n\n}\n\n\n\n\n\nThe OpenMP parallelism is coarse-grained; rather than constructing a large\n\nBox\n from \nmfi.validbox()\n, it constructs a smaller \nBox\n from\n\nmfi.tilebox()\n. The metadata format remains unchanged, allowing the Fortran\nfunction to remain unchanged as well.", 
            "title": "Code Layout"
        }, 
        {
            "location": "/case_studies/amr/code_layout/#code-layout", 
            "text": "The typical form of a BoxLib application is a C++ \"driver\" code which manages\nthe boxes on the domain, and calls Fortran kernels in a loop over the boxes\nowned by each MPI process. An example of this layout is shown below:  // Advance the solution one grid at a time  for   (   MFIter   mfi ( old_phi );   mfi . isValid ();   ++ mfi   )  { \n   const   Box   bx   =   mfi . validbox (); \n\n   update_phi ( old_phi [ mfi ]. dataPtr (), \n              new_phi [ mfi ]. dataPtr (), \n              ng_p , \n              flux [ 0 ][ mfi ]. dataPtr (), \n              flux [ 1 ][ mfi ]. dataPtr (), \n              flux [ 2 ][ mfi ]. dataPtr (), \n              ng_f ,   bx . loVect (),   bx . hiVect (),   dx [ 0 ],   dt ); \n   }  }   Here the  MFIter  object is an iterator over boxes owned by an MPI process. The Box  object contains the geometric metadata describing a particular box, e.g.,\nthe indices of the lower and upper corners. The variables  old_phi ,  new_phi ,\nand  flux  contain pointers to the arrays which contain the floating point data\non the grid. The  update_phi  function is a Fortran function which uses the\ndata from the  Box  object to construct 3-D loops over the appropriate section\nof the three floating-point arrays. The function may look like the following:  subroutine  update_phi ( phiold ,   phinew ,   ng_p ,   fluxx ,   fluxy ,   fluxz ,   ng_f ,   lo ,   hi ,   dx ,   dt )   bind ( C ,   name = update_phi ) \n\n   integer            ::   lo ( 3 ),   hi ( 3 ),   ng_p ,   ng_f \n   double precision   ::   phiold ( lo ( 1 ) - ng_p : hi ( 1 ) + ng_p , lo ( 2 ) - ng_p : hi ( 2 ) + ng_p , lo ( 3 ) - ng_p : hi ( 3 ) + ng_p ) \n   double precision   ::   phinew ( lo ( 1 ) - ng_p : hi ( 1 ) + ng_p , lo ( 2 ) - ng_p : hi ( 2 ) + ng_p , lo ( 3 ) - ng_p : hi ( 3 ) + ng_p ) \n   double precision   ::    fluxx ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f + 1 , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f ) \n   double precision   ::    fluxy ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f + 1 , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f ) \n   double precision   ::    fluxz ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f + 1 ) \n   double precision   ::   dx ,   dt \n\n   integer  i , j , k \n\n   do  k = lo ( 3 ), hi ( 3 ) \n      do  j = lo ( 2 ), hi ( 2 ) \n         do  i = lo ( 1 ), hi ( 1 ) \n\n            phinew ( i , j , k )   =   phiold ( i , j , k )   +   dt   *   \n                 (   fluxx ( i + 1 , j , k ) - fluxx ( i , j , k )   \n                 + fluxy ( i , j + 1 , k ) - fluxy ( i , j , k )   \n                 + fluxz ( i , j , k + 1 ) - fluxz ( i , j , k )   )   /   dx \n\n         end do       end do    end do   The Fortran function constructs the appropriate \"view\" into each box using the\ndata from the  Box  object from the C++ function, as well as from the number of\nghost zones ( ng_p  for  old_phi  and  new_phi , and  ng_f  for  flux ).  The above example demonstrates pure MPI parallelism; the analogous C++ code\nwhich uses OpenMP tiling as described above would look like the following:  // Advance the solution one grid at a time  #ifdef _OPENMP  #pragma omp parallel  #endif  for   (   MFIter   mfi ( old_phi , true );   mfi . isValid ();   ++ mfi   )  { \n   const   Box   tbx   =   mfi . tilebox (); \n\n   update_phi ( old_phi [ mfi ]. dataPtr (), \n              new_phi [ mfi ]. dataPtr (), \n              ng_p , \n              flux [ 0 ][ mfi ]. dataPtr (), \n              flux [ 1 ][ mfi ]. dataPtr (), \n              flux [ 2 ][ mfi ]. dataPtr (), \n              ng_f ,   tbx . loVect (),   tbx . hiVect (),   dx [ 0 ],   dt ); \n   }  }   The OpenMP parallelism is coarse-grained; rather than constructing a large Box  from  mfi.validbox() , it constructs a smaller  Box  from mfi.tilebox() . The metadata format remains unchanged, allowing the Fortran\nfunction to remain unchanged as well.", 
            "title": "Code Layout"
        }, 
        {
            "location": "/case_studies/amr/multigrid/", 
            "text": "Geometric Multigrid\n\n\nMany problems encountered in BoxLib applications require solutions to linear\nsystem, e.g., elliptic partial differential equations such as the Poisson\nequation for self-gravity, and the diffusion equation. BoxLib therefore\nincludes geometric multigrid solvers for solving problems which use both\ncell-centered and nodal data. For this project, we have focused on the\ncell-centered solver due to its relative simplicity compared to the nodal\nsolver.\n\n\nGeometric multigrid is an iterative method for solving linear problems which\ncontains roughly 4 steps:\n\n\n\n\nrelaxation\n\n\nrestriction\n\n\nprolongation\n\n\ncoarse-grid linear solve (either approximate or exact)\n\n\n\n\nAlthough here we will not discuss the details of the geometric multigrid\nmethod, we summarize each of these steps below as they pertain to computational\nalgorithms.\n\n\nRelaxation\n\n\nA relaxation consists of one or more iterations of an approximate solution to\nthe system of linear equations. In geometric multigrid, common algorithms used\nhere include Jacobi and Gauss-Seidel. By default, the BoxLib solver uses a\nvariation on Gauss-Seidel called Gauss-Seidel red-black (\"GSRB\"). GSRB deviates\nfrom the original Gauss-Seidel method by exploiting a symmetry in the data\ndependence among matrix elements, such that an update sweep of all matrix\nelements follows a stride-2 pattern rather than stride-1. (This property\nmanifests in the innermost loop of the kernel shown below).\n\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n     \nioff\n \n=\n \nMOD\n(\nlo\n(\n1\n)\n \n+\n \nj\n \n+\n \nk\n \n+\n \nredblack\n,\n2\n)\n\n     \ndo \ni\n \n=\n \nlo\n(\n1\n)\n \n+\n \nioff\n,\nhi\n(\n1\n),\n2\n\n        \ngamma\n \n=\n \nalpha\n*\na\n(\ni\n,\nj\n,\nk\n)\n \n\n              \n+\n   \ndhx\n*\n(\nbX\n(\ni\n,\nj\n,\nk\n)\n+\nbX\n(\ni\n+\n1\n,\nj\n,\nk\n))\n \n\n              \n+\n   \ndhy\n*\n(\nbY\n(\ni\n,\nj\n,\nk\n)\n+\nbY\n(\ni\n,\nj\n+\n1\n,\nk\n))\n \n\n              \n+\n   \ndhz\n*\n(\nbZ\n(\ni\n,\nj\n,\nk\n)\n+\nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n))\n\n\n        \ng_m_d\n \n=\n \ngamma\n \n\n              \n-\n \n(\ndhx\n*\n(\nbX\n(\ni\n,\nj\n,\nk\n)\n*\ncf0\n \n+\n \nbX\n(\ni\n+\n1\n,\nj\n,\nk\n)\n*\ncf3\n)\n \n\n              \n+\n  \ndhy\n*\n(\nbY\n(\ni\n,\nj\n,\nk\n)\n*\ncf1\n \n+\n \nbY\n(\ni\n,\nj\n+\n1\n,\nk\n)\n*\ncf4\n)\n \n\n              \n+\n  \ndhz\n*\n(\nbZ\n(\ni\n,\nj\n,\nk\n)\n*\ncf2\n \n+\n \nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n)\n*\ncf5\n))\n \n\n\n        \nrho\n \n=\n \ndhx\n*\n(\n \nbX\n(\ni\n  \n,\nj\n,\nk\n)\n*\nphi\n(\ni\n-\n1\n,\nj\n,\nk\n)\n \n\n            \n+\n       \nbX\n(\ni\n+\n1\n,\nj\n,\nk\n)\n*\nphi\n(\ni\n+\n1\n,\nj\n,\nk\n)\n \n)\n \n\n            \n+\n \ndhy\n*\n(\n \nbY\n(\ni\n,\nj\n  \n,\nk\n)\n*\nphi\n(\ni\n,\nj\n-\n1\n,\nk\n)\n \n\n            \n+\n       \nbY\n(\ni\n,\nj\n+\n1\n,\nk\n)\n*\nphi\n(\ni\n,\nj\n+\n1\n,\nk\n)\n \n)\n \n\n            \n+\n \ndhz\n*\n(\n \nbZ\n(\ni\n,\nj\n,\nk\n  \n)\n*\nphi\n(\ni\n,\nj\n,\nk\n-\n1\n)\n \n\n            \n+\n       \nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n)\n*\nphi\n(\ni\n,\nj\n,\nk\n+\n1\n)\n \n)\n \n\n\n        \nres\n \n=\n  \nrhs\n(\ni\n,\nj\n,\nk\n)\n \n-\n \n(\ngamma\n*\nphi\n(\ni\n,\nj\n,\nk\n)\n \n-\n \nrho\n)\n\n        \nphi\n(\ni\n,\nj\n,\nk\n)\n \n=\n \nphi\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nomega\n/\ng_m_d\n \n*\n \nres\n\n     \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\n\nThe algorithm above uses a 7-point cell-centered discretization of the 3-D\nvariable-coefficient Helmholtz operator. The diffusion operator is one type of\nHelmholtz operator; the Laplace operator, which appears in the Poisson equation\nfor self-gravity, is a simplified version, with constant coefficients.\n\n\nThe GSRB method for a 7-point discretization of the Helmholtz operator exhibits\na low arithmetic intensity, requiring several non-contiguous loads from memory\nto evaluate the operator.\n\n\nThe relaxation step and the coarse grid solve (discussed belowed) often feature\nsimilar computational and data access patterns, because both are effectively\ndoing the same thing - solving a linear system. The primary difference between\nthem is that the relaxation method applies the iterative kernel only a handful\nof times, whereas the coarse grid solve often iterates all the way to\nconvergence.\n\n\nRestriction\n\n\nDuring a restriction, the value of a field on a fine grid is approximated on a\ncoarser grid. This is typically done by averaging values of the field on fine\ngrid points onto the corresponding grid points on the coarse grid. In BoxLib,\nthe algorithm is the following:\n\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \nk2\n \n=\n \n2\n*\nk\n\n  \nk2p1\n \n=\n \nk2\n \n+\n \n1\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n    \nj2\n \n=\n \n2\n*\nj\n\n    \nj2p1\n \n=\n \nj2\n \n+\n \n1\n\n    \ndo \ni\n \n=\n \nlo\n(\n1\n),\n \nhi\n(\n1\n)\n\n      \ni2\n \n=\n \n2\n*\ni\n\n      \ni2p1\n \n=\n \ni2\n \n+\n \n1\n\n      \nc\n(\ni\n,\nj\n,\nk\n)\n \n=\n  \n(\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n \n+\n \nf\n(\ni2\n,\nj2p1\n,\nk2\n  \n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n \n+\n \nf\n(\ni2\n,\nj2\n  \n,\nk2\n  \n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n \n+\n \nf\n(\ni2\n,\nj2p1\n,\nk2p1\n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n \n+\n \nf\n(\ni2\n,\nj2\n  \n,\nk2p1\n)\n\n\n$\n                 \n)\n*\neighth\n\n    \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\n\nwhere \nf\n is the field on the fine grid and \nc\n is the field on the coarse\ngrid. (This multigrid solver always coarsens grids by factors of two in each\ndimension.) For each evaluation of a coarse grid point, the algorithm must load\n8 values from the fine grid. However, there is significant memory locality in\nthis algorithm, as many of the fine grid points for coarse grid point\n\nc(i,j,k)\n also contribute to the point \nc(i+1,j,k)\n.\n\n\nProlongation\n\n\nProlongation (also called interpolation) is the opposite of restriction: one\napproximates the value of a field on a coarse grid on a finer grid. The\nprolongation kernel in the BoxLib solver is as follows:\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \nk2\n \n=\n \n2\n*\nk\n\n  \nk2p1\n \n=\n \nk2\n \n+\n \n1\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n    \nj2\n \n=\n \n2\n*\nj\n\n    \nj2p1\n \n=\n \nj2\n \n+\n \n1\n\n    \ndo \ni\n \n=\n \nlo\n(\n1\n),\n \nhi\n(\n1\n)\n\n      \ni2\n \n=\n \n2\n*\ni\n\n      \ni2p1\n \n=\n \ni2\n \n+\n \n1\n\n\n      \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n\n      \nf\n(\ni2\n  \n,\nj2p1\n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2p1\n,\nk2\n  \n)\n\n      \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n\n      \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2\n  \n)\n\n      \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n\n      \nf\n(\ni2\n  \n,\nj2p1\n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2p1\n,\nk2p1\n)\n\n      \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n\n      \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2p1\n)\n\n\n    \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\nIn 3-D, the same value on the coarse grid contributes equally to eight\nneighboring points in the fine grid. (The symmetry arises from the constraint\nin the solver that the cells must be cubic.)\n\n\nExact linear solve\n\n\nThe multigrid solver in BoxLib recursively coarsens grids until the grid\nreaches a sufficiently small size, often \n\\(2^3\\)\n if the problem domain is cubic.\nOn the coarsest grid, the solver then solves the linear system exactly, before\npropagating the solution back up to finer grids. The solution algorithm chosen\nfor this step is rarely influential on the overall performance of the multigrid\nalgorithm, because the problem size at the coarsest grid is so small. In\nBoxLib, the default coarse grid solver algorithm is BiCGSTAB, a variation on\nthe conjugate-gradient iterative method.", 
            "title": "Geometric Multigrid"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#geometric-multigrid", 
            "text": "Many problems encountered in BoxLib applications require solutions to linear\nsystem, e.g., elliptic partial differential equations such as the Poisson\nequation for self-gravity, and the diffusion equation. BoxLib therefore\nincludes geometric multigrid solvers for solving problems which use both\ncell-centered and nodal data. For this project, we have focused on the\ncell-centered solver due to its relative simplicity compared to the nodal\nsolver.  Geometric multigrid is an iterative method for solving linear problems which\ncontains roughly 4 steps:   relaxation  restriction  prolongation  coarse-grid linear solve (either approximate or exact)   Although here we will not discuss the details of the geometric multigrid\nmethod, we summarize each of these steps below as they pertain to computational\nalgorithms.", 
            "title": "Geometric Multigrid"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#relaxation", 
            "text": "A relaxation consists of one or more iterations of an approximate solution to\nthe system of linear equations. In geometric multigrid, common algorithms used\nhere include Jacobi and Gauss-Seidel. By default, the BoxLib solver uses a\nvariation on Gauss-Seidel called Gauss-Seidel red-black (\"GSRB\"). GSRB deviates\nfrom the original Gauss-Seidel method by exploiting a symmetry in the data\ndependence among matrix elements, such that an update sweep of all matrix\nelements follows a stride-2 pattern rather than stride-1. (This property\nmanifests in the innermost loop of the kernel shown below).  do  k   =   lo ( 3 ),   hi ( 3 ) \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n      ioff   =   MOD ( lo ( 1 )   +   j   +   k   +   redblack , 2 ) \n      do  i   =   lo ( 1 )   +   ioff , hi ( 1 ), 2 \n         gamma   =   alpha * a ( i , j , k )   \n               +     dhx * ( bX ( i , j , k ) + bX ( i + 1 , j , k ))   \n               +     dhy * ( bY ( i , j , k ) + bY ( i , j + 1 , k ))   \n               +     dhz * ( bZ ( i , j , k ) + bZ ( i , j , k + 1 )) \n\n         g_m_d   =   gamma   \n               -   ( dhx * ( bX ( i , j , k ) * cf0   +   bX ( i + 1 , j , k ) * cf3 )   \n               +    dhy * ( bY ( i , j , k ) * cf1   +   bY ( i , j + 1 , k ) * cf4 )   \n               +    dhz * ( bZ ( i , j , k ) * cf2   +   bZ ( i , j , k + 1 ) * cf5 ))   \n\n         rho   =   dhx * (   bX ( i    , j , k ) * phi ( i - 1 , j , k )   \n             +         bX ( i + 1 , j , k ) * phi ( i + 1 , j , k )   )   \n             +   dhy * (   bY ( i , j    , k ) * phi ( i , j - 1 , k )   \n             +         bY ( i , j + 1 , k ) * phi ( i , j + 1 , k )   )   \n             +   dhz * (   bZ ( i , j , k    ) * phi ( i , j , k - 1 )   \n             +         bZ ( i , j , k + 1 ) * phi ( i , j , k + 1 )   )   \n\n         res   =    rhs ( i , j , k )   -   ( gamma * phi ( i , j , k )   -   rho ) \n         phi ( i , j , k )   =   phi ( i , j , k )   +   omega / g_m_d   *   res \n      end do    end do  end do   The algorithm above uses a 7-point cell-centered discretization of the 3-D\nvariable-coefficient Helmholtz operator. The diffusion operator is one type of\nHelmholtz operator; the Laplace operator, which appears in the Poisson equation\nfor self-gravity, is a simplified version, with constant coefficients.  The GSRB method for a 7-point discretization of the Helmholtz operator exhibits\na low arithmetic intensity, requiring several non-contiguous loads from memory\nto evaluate the operator.  The relaxation step and the coarse grid solve (discussed belowed) often feature\nsimilar computational and data access patterns, because both are effectively\ndoing the same thing - solving a linear system. The primary difference between\nthem is that the relaxation method applies the iterative kernel only a handful\nof times, whereas the coarse grid solve often iterates all the way to\nconvergence.", 
            "title": "Relaxation"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#restriction", 
            "text": "During a restriction, the value of a field on a fine grid is approximated on a\ncoarser grid. This is typically done by averaging values of the field on fine\ngrid points onto the corresponding grid points on the coarse grid. In BoxLib,\nthe algorithm is the following:  do  k   =   lo ( 3 ),   hi ( 3 ) \n   k2   =   2 * k \n   k2p1   =   k2   +   1 \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n     j2   =   2 * j \n     j2p1   =   j2   +   1 \n     do  i   =   lo ( 1 ),   hi ( 1 ) \n       i2   =   2 * i \n       i2p1   =   i2   +   1 \n       c ( i , j , k )   =    (  $                   +   f ( i2p1 , j2p1 , k2    )   +   f ( i2 , j2p1 , k2    )  $                   +   f ( i2p1 , j2    , k2    )   +   f ( i2 , j2    , k2    )  $                   +   f ( i2p1 , j2p1 , k2p1 )   +   f ( i2 , j2p1 , k2p1 )  $                   +   f ( i2p1 , j2    , k2p1 )   +   f ( i2 , j2    , k2p1 )  $                   ) * eighth \n     end do    end do  end do   where  f  is the field on the fine grid and  c  is the field on the coarse\ngrid. (This multigrid solver always coarsens grids by factors of two in each\ndimension.) For each evaluation of a coarse grid point, the algorithm must load\n8 values from the fine grid. However, there is significant memory locality in\nthis algorithm, as many of the fine grid points for coarse grid point c(i,j,k)  also contribute to the point  c(i+1,j,k) .", 
            "title": "Restriction"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#prolongation", 
            "text": "Prolongation (also called interpolation) is the opposite of restriction: one\napproximates the value of a field on a coarse grid on a finer grid. The\nprolongation kernel in the BoxLib solver is as follows: do  k   =   lo ( 3 ),   hi ( 3 ) \n   k2   =   2 * k \n   k2p1   =   k2   +   1 \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n     j2   =   2 * j \n     j2p1   =   j2   +   1 \n     do  i   =   lo ( 1 ),   hi ( 1 ) \n       i2   =   2 * i \n       i2p1   =   i2   +   1 \n\n       f ( i2p1 , j2p1 , k2    )   =   c ( i , j , k )   +   f ( i2p1 , j2p1 , k2    ) \n       f ( i2    , j2p1 , k2    )   =   c ( i , j , k )   +   f ( i2    , j2p1 , k2    ) \n       f ( i2p1 , j2    , k2    )   =   c ( i , j , k )   +   f ( i2p1 , j2    , k2    ) \n       f ( i2    , j2    , k2    )   =   c ( i , j , k )   +   f ( i2    , j2    , k2    ) \n       f ( i2p1 , j2p1 , k2p1 )   =   c ( i , j , k )   +   f ( i2p1 , j2p1 , k2p1 ) \n       f ( i2    , j2p1 , k2p1 )   =   c ( i , j , k )   +   f ( i2    , j2p1 , k2p1 ) \n       f ( i2p1 , j2    , k2p1 )   =   c ( i , j , k )   +   f ( i2p1 , j2    , k2p1 ) \n       f ( i2    , j2    , k2p1 )   =   c ( i , j , k )   +   f ( i2    , j2    , k2p1 ) \n\n     end do    end do  end do   In 3-D, the same value on the coarse grid contributes equally to eight\nneighboring points in the fine grid. (The symmetry arises from the constraint\nin the solver that the cells must be cubic.)", 
            "title": "Prolongation"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#exact-linear-solve", 
            "text": "The multigrid solver in BoxLib recursively coarsens grids until the grid\nreaches a sufficiently small size, often  \\(2^3\\)  if the problem domain is cubic.\nOn the coarsest grid, the solver then solves the linear system exactly, before\npropagating the solution back up to finer grids. The solution algorithm chosen\nfor this step is rarely influential on the overall performance of the multigrid\nalgorithm, because the problem size at the coarsest grid is so small. In\nBoxLib, the default coarse grid solver algorithm is BiCGSTAB, a variation on\nthe conjugate-gradient iterative method.", 
            "title": "Exact linear solve"
        }, 
        {
            "location": "/case_studies/gw/", 
            "text": "", 
            "title": "GW Kenerels"
        }, 
        {
            "location": "/case_studies/nek/", 
            "text": "", 
            "title": "NekBone"
        }, 
        {
            "location": "/case_studies/md/", 
            "text": "", 
            "title": "MD"
        }, 
        {
            "location": "/demo/demo/", 
            "text": "Welcome to NERSC\n\n\nWelcome to the National Energy Research Scientific Computing Center, a high performance scientific computing center.\nThis document will guide you through the basics of using NERSC\u2019s supercomputers, storage systems, and services.\n\n\nWhat is NERSC?\n\n\nNERSC provides High Performance Computing and Storage facilities and support for research sponsored by, and of interest to, the U.S. Department of Energy Office of Science. NERSC has the unique programmatic role of supporting all six Office of Science program offices: Advanced Scientific Computing Research, Basic Energy Sciences, Biological and Environmental Research, Fusion Energy Sciences, High Energy Physics, and Nuclear Physics. Scientists who have been awarded research funding by any of the offices are eligible to apply for an allocation of NERSC time. Additional awards may be given to non-DOE funded project teams whose research is aligned with the Office of Science's mission. Allocations of time and storage are made by DOE.\n\n\nNERSC is a national center, organizationally part of Lawrence Berkeley National Laboratory in Berkeley, CA. NERSC staff and facilities are primarily located at Berkeley Lab's Shyh Wang Hall on the Berkeley Lab campus.\n\n\nExternal links\n\n\n\n\nOLCF\n\n\nALCF\n\n\nNERSC\n\n\n\n\nInternal links\n\n\n\n\nPortability definition\n\n\n\n\nTables:\n\n\n\n\n\n\n\n\nSystem Type\n\n\nCray XC40\n\n\n\n\n\n\n\n\n\n\nTheoretical Peak Performance (System)\n\n\n31.4 PFlops\n\n\n\n\n\n\nTheoretical Peak Performance (Haswell nodes)\n\n\n2.3 PFlops\n\n\n\n\n\n\nTheoretical Peak Performance (Xeon Phi nodes)\n\n\n29.1 PFlops\n\n\n\n\n\n\nCompute Nodes (Haswell)\n\n\n2,388\n\n\n\n\n\n\n\n\nInclude scripts/ source code\n\n\nThis site supports an include extension to Markdown.\n\n\nOne way to run a pure MPI job on Cori is\n\n\n#!/bin/bash -l\n\n\n#SBATCH -p debug\n\n\n#SBATCH -N 64\n\n\n#SBATCH -t 00:20:00\n\n\n#SBATCH -J my_job\n\n\n#SBATCH -L SCRATCH\n\n\n#SBATCH -C haswell\n\n\n\n# an extra -c 2 flag is optional for fully packed pure MPI\n\nsrun -n \n2048\n ./mycode.exe\n\n\n\n\n\n\nWarning\n\n\nThe \n-c\n and \n--cpu_bind=\n options for \nsrun\n are \nrequired\n for hybrid jobs or jobs which do not utilize all physical cores \n\n\n\n\nSome source code\n\n\nInstrumented C code to measure AI\n\n\n// Code must be built with appropriate paths for VTune include file (ittnotify.h) and library (-littnotify)\n\n\n#include\n \nittnotify.h\n\n\n\n__SSC_MARK\n(\n0x111\n);\n \n// start SDE tracing, note it uses 2 underscores\n\n\n__itt_resume\n();\n \n// start VTune, again use 2 underscores\n\n\n\nfor\n \n(\nk\n=\n0\n;\n \nk\nNTIMES\n;\n \nk\n++\n)\n \n{\n\n \n#pragma omp parallel for\n\n \nfor\n \n(\nj\n=\n0\n;\n \nj\nSTREAM_ARRAY_SIZE\n;\n \nj\n++\n)\n\n \na\n[\nj\n]\n \n=\n \nb\n[\nj\n]\n+\nscalar\n*\nc\n[\nj\n];\n\n\n}\n\n\n\n__itt_pause\n();\n \n// stop VTune\n\n\n__SSC_MARK\n(\n0x222\n);\n \n// stop SDE tracing\n\n\n\n\n\nAnd some totally unrelated python code\n\n\ndef\n \ncount_cross_connections\n(\ncounts\n):\n\n    \nCounts is a list of the number of nodes in each (non-zero) group\n\n    \nreturn\n \nsum\n(\n \nx\n[\n0\n]\n*\nx\n[\n1\n]\n \nfor\n \nx\n \nin\n \nitertools\n.\ncombinations\n(\ncounts\n,\n \n2\n)\n \n)\n\n\n\n\n\nLaTex support\n\n\n\\[\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\\]\nfrom:\n\n\n$$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$\n\n\n\n\nImages\n\n\n\n\nInline HTML\n\n\nThis is probably best avoided if possible, but it works.\n\n\n\n  \nDefinition list\n\n  \nIs something people use sometimes.\n\n\n  \nMarkdown in HTML\n\n  \nDoes *not* work **very** well. Use HTML \ntags\n.", 
            "title": "Demo"
        }, 
        {
            "location": "/demo/demo/#welcome-to-nersc", 
            "text": "Welcome to the National Energy Research Scientific Computing Center, a high performance scientific computing center.\nThis document will guide you through the basics of using NERSC\u2019s supercomputers, storage systems, and services.", 
            "title": "Welcome to NERSC"
        }, 
        {
            "location": "/demo/demo/#what-is-nersc", 
            "text": "NERSC provides High Performance Computing and Storage facilities and support for research sponsored by, and of interest to, the U.S. Department of Energy Office of Science. NERSC has the unique programmatic role of supporting all six Office of Science program offices: Advanced Scientific Computing Research, Basic Energy Sciences, Biological and Environmental Research, Fusion Energy Sciences, High Energy Physics, and Nuclear Physics. Scientists who have been awarded research funding by any of the offices are eligible to apply for an allocation of NERSC time. Additional awards may be given to non-DOE funded project teams whose research is aligned with the Office of Science's mission. Allocations of time and storage are made by DOE.  NERSC is a national center, organizationally part of Lawrence Berkeley National Laboratory in Berkeley, CA. NERSC staff and facilities are primarily located at Berkeley Lab's Shyh Wang Hall on the Berkeley Lab campus.", 
            "title": "What is NERSC?"
        }, 
        {
            "location": "/demo/demo/#external-links", 
            "text": "OLCF  ALCF  NERSC", 
            "title": "External links"
        }, 
        {
            "location": "/demo/demo/#internal-links", 
            "text": "Portability definition   Tables:     System Type  Cray XC40      Theoretical Peak Performance (System)  31.4 PFlops    Theoretical Peak Performance (Haswell nodes)  2.3 PFlops    Theoretical Peak Performance (Xeon Phi nodes)  29.1 PFlops    Compute Nodes (Haswell)  2,388", 
            "title": "Internal links"
        }, 
        {
            "location": "/demo/demo/#include-scripts-source-code", 
            "text": "This site supports an include extension to Markdown.  One way to run a pure MPI job on Cori is  #!/bin/bash -l  #SBATCH -p debug  #SBATCH -N 64  #SBATCH -t 00:20:00  #SBATCH -J my_job  #SBATCH -L SCRATCH  #SBATCH -C haswell  # an extra -c 2 flag is optional for fully packed pure MPI \nsrun -n  2048  ./mycode.exe   Warning  The  -c  and  --cpu_bind=  options for  srun  are  required  for hybrid jobs or jobs which do not utilize all physical cores", 
            "title": "Include scripts/ source code"
        }, 
        {
            "location": "/demo/demo/#some-source-code", 
            "text": "Instrumented C code to measure AI  // Code must be built with appropriate paths for VTune include file (ittnotify.h) and library (-littnotify)  #include   ittnotify.h  __SSC_MARK ( 0x111 );   // start SDE tracing, note it uses 2 underscores  __itt_resume ();   // start VTune, again use 2 underscores  for   ( k = 0 ;   k NTIMES ;   k ++ )   { \n  #pragma omp parallel for \n  for   ( j = 0 ;   j STREAM_ARRAY_SIZE ;   j ++ ) \n  a [ j ]   =   b [ j ] + scalar * c [ j ];  }  __itt_pause ();   // stop VTune  __SSC_MARK ( 0x222 );   // stop SDE tracing   And some totally unrelated python code  def   count_cross_connections ( counts ): \n     Counts is a list of the number of nodes in each (non-zero) group \n     return   sum (   x [ 0 ] * x [ 1 ]   for   x   in   itertools . combinations ( counts ,   2 )   )", 
            "title": "Some source code"
        }, 
        {
            "location": "/demo/demo/#latex-support", 
            "text": "\\[\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\\] from:  $$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$", 
            "title": "LaTex support"
        }, 
        {
            "location": "/demo/demo/#images", 
            "text": "", 
            "title": "Images"
        }, 
        {
            "location": "/demo/demo/#inline-html", 
            "text": "This is probably best avoided if possible, but it works.  \n   Definition list \n   Is something people use sometimes. \n\n   Markdown in HTML \n   Does *not* work **very** well. Use HTML  tags .", 
            "title": "Inline HTML"
        }
    ]
}